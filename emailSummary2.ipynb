{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTqbsZBVnjgu",
        "outputId": "f99b1ec7-6ec5-43d1-a444-4ec1ea00ac0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summaries saved to 'thread_summaries2.json'\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# Load BART tokenizer and model\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
        "\n",
        "# Load the dataset\n",
        "with open('test.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Group emails by thread_id\n",
        "threads = {}\n",
        "for email in data:\n",
        "    thread_id = email['thread_id']\n",
        "    if thread_id not in threads:\n",
        "        threads[thread_id] = {'subject': email['subject'], 'emails': []}\n",
        "    threads[thread_id]['emails'].append(email['body'])\n",
        "\n",
        "# Summarize each thread\n",
        "thread_summaries = {}\n",
        "for thread_id, thread_data in threads.items():\n",
        "    # Concatenate all email bodies within the thread\n",
        "    thread_body = \" \".join(thread_data['emails'])\n",
        "\n",
        "    # Tokenize and summarize the thread body\n",
        "    inputs = tokenizer(thread_body, return_tensors='pt', max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=300, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    # Remove \\u00a0 and \\ from the summary\n",
        "    summary = summary.replace('\\u00a0', '')\n",
        "\n",
        "    # Store the thread summary along with the subject\n",
        "    thread_summaries[thread_id] = {'subject': thread_data['subject'], 'summary': summary}\n",
        "\n",
        "# Save thread summaries to a file\n",
        "with open('thread_summaries2.json', 'w') as f:\n",
        "    json.dump(thread_summaries, f, indent=4)\n",
        "\n",
        "print(\"Summaries saved to 'thread_summaries2.json'\")\n"
      ]
    }
  ]
}